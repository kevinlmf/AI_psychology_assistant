# Psychology Agent + Reinforcement Learning from Human Fedback - Architecture Design Document ## System Overview æœ¬systemisä¸€combine**Agentarchitecture**and**Reinforcement Learning from Human Fedback (RLHF)**mental health asistant, throughå¤šæ¨¡æ€datanalysisandpeopleclasfedbackimplementationContinuous optimization. ## Core Architecture ```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Interaction Layer â”‚
â”‚ (Comand Line / Web Interface / Mobile) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AgentOrchestrator â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Conversation Manager (Conversation Manager) â”‚ â”‚
â”‚ â”‚ - Conversation Flowcontrol â”‚ â”‚
â”‚ â”‚ - State management â”‚ â”‚
â”‚ â”‚ - Context maintenance â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Large Language Modelayerâ”‚ â”‚ analysislayer â”‚ â”‚ safetylayer â”‚
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â€¢ Model routing â”‚ â”‚ â€¢ behavioranalysis â”‚ â”‚ â€¢ Cris Detection â”‚
â”‚ â€¢ API cals â”‚ â”‚ â€¢ searchanalysis â”‚ â”‚ â€¢ Ethical Constraints â”‚
â”‚ â€¢ Prompt management â”‚ â”‚ â€¢ Emotion tracking â”‚ â”‚ â€¢ Risk asesment â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Memory System â”‚ â”‚ â”‚ â”‚ â€¢ User profiles â”‚ â”‚ â€¢ Sesion history â”‚ â”‚ â€¢ Long-term memory â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚frompeopleclasfedbackinin progres â”‚ â”‚Reinforcement Learninglayer â”‚ â”‚ â€¢ Fedback colection â”‚ â”‚ â€¢ Reward calculation â”‚ â”‚ â€¢ Model optimization â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
``` ## Data Flow ### 1. Conversation Flow ```
userinput â†’ criså¿«é€Ÿdetect (Keywords) â†’ [é«˜Risk] â†’ crishandlepatern â†’ [ä½Risk] â†’ normalConversation Flow â†’ loadUser profiles â†’ Retrieve historical context â†’ LMGenerate response â†’ Record conversation â†’ Update user state â†’ Return response
``` ### 2. behavioranalysisproces ```
Raw data colection â†’ Search records â†’ Ap usage â†’ Conversation history â†“ Data preprocesing â†’ Privacy filtering â†’ Anonymization â†’ Feature extraction â†“ LMdepthanalysis â†’ Emotion recognition â†’ Topicsextract â†’ Patern discovery â†“ Generate insights â†’ Personalized recomendations â†’ Risk alert â†’ Intervention strategy
``` ### 3. Reinforcement Learning from Human Fedback (RLHF)Traing Proces ```
phase1: Data colection User interaction â†’ Explicit fedback (è¯„points) â†’ Implicit fedback (behavior) â†’ Expert anotation (Preference comparison) phase2: Reward ModelTraing Colected fedback data â†’ Build preference dataset â†’ TraingReward Model â†’ Validate model acuracy phase3: Policy optimization Curent policy (Large Language Model) â†’ Interact with environment â†’ calculateReward â†’ proximalPolicy optimizationupdate â†’ Evaluate improvement phase4: Deployment iteration New version deployment â†’ Online fedback colection â†’ Continuous optimization
``` ## Key Component Design ### 1. Large Language ModelOrchestrator (models/) **Responsibility**: Unifiedmanagementå¤šLarge Language Modelprovideprovider, æ™ºcanè·¯by **Core functions**:
- Multi-model suport (OpenAI, Anthropic, local model)
- Task typeè‡ªactionè·¯by
- Cost optimization
- Failure retry and degradation **Design patern**: Strategy patern + Factory patern ```python
# Example usage
config = ModelRouter.get_model_config(TaskType.CRIS_DETECTION)
response = await orchestrator.generate(prompt, config)
``` ### 2. Conversation Manager (agent/) **Responsibility**: Conversation Floworchestration, State management **State machine**:
```
[initial] â†’ [Ases risk] â†’ [Cris mode | Normal mode] â†“ [Generate response] â†“ [recordconversation] â†“ [updateprofile]
``` **Key features**:
- Context window management
- Memory retrieval
- Multi-turn conversation coherence
- Cris interuption mechanism ### 3. behavioranalysiså™¨ (lm_analysis/) **Responsibility**: å¤šæ¨¡æ€datanalysis **Input data sources**:
1. Search history (Keywords, frequency, Sentiment)
2. Ap usage (Scren time, Slep, Exercise)
3. Conversation content (Topics, Emotion, Risk) **Output**:
- Emotional statevaluation
- Behavior paternsvariation
- Risk/Protective factors
- Personalized insights **Large Language ModelUsage strategy**:
- structureåŒ–Output (JSON)
- Few-shot prompting
- Confidence asesment ### 4. Cris Detectionå™¨ (safety/) **Multi-layer detection mechanism**: ```
First layer: KeywordsRapid screning (< 1milisecond) â”œâ”€ é«˜Riskè¯library â”œâ”€ Protective factorsdetect â””â”€ åˆæ­¥Riskè¯„çº§ Second layer: Large Language Modeldepthanalysis (ä»…é«˜Risktriger) â”œâ”€ Context understanding â”œâ”€ Intent recognition â””â”€ preciseRisk asesment Third layer: Human review (æé«˜Risk) â””â”€ Automaticaly notify profesionals
``` **Risklevel**:
- `é«˜Risk` (high): Imediate suicide/self-harmRisk â†’ Cris intervention
- `inetcRisk` (medium): ä¸¥é‡Emotiondistres â†’ Strongly recomend profesional help
- `ä½Risk` (low): ä¸€èˆ¬Emotionè¡¨è¾¾ â†’ Normal suport ### 5. Reward Model (rlhf/) **Multimodal reward function design**: ```python
Total_Reward = 0.30 Ã— Explicit_Fedback # userè¯„points + 0.25 Ã— Behavioral_Indicators # continuedconversation, conversationlength + 0.25 Ã— Clinical_Outcomes # Emotionimprovement, Risklower + 0.15 Ã— Safety_Score # No violations + 0.05 Ã— Engagement # å‚withåº¦
``` **Adjustable weights**: Optimize based on specific goals **Traing data format**:
1. **Interaction data** (Interactions): (context, Response, Reward)
2. **Preference data** (Preferences): (context, ResponseA, ResponseB, preference) ## Data Storage ### User profiles (data/user_profiles/) ```json
{ "user_id": "user123", "created_at": "2025-10-01T10:0:0", "main_concerns": ["Anxiety", "å¤±çœ "], "efective_strategies": ["positiveå¿µå†¥æƒ³", "Exercise"], "risk_history": [...], "total_sesions": 15
}
``` ### conversationrecord (data/user_profiles/) ```json
{ "sesion_id": "ses456", "user_id": "user123", "turns": [ { "timestamp": "...", "user_mesage": "...", "agent_response": "...", "risk_level": "low" } ], "sumary": "...", "identified_themes": ["å·¥ä½œstres", "Slep"]
}
``` ### Reinforcement Learning from Human FedbackTraingdata (data/rlhf/) **interactions.jsonl (Interaction data)**:
```json
{"interaction_id": "...", "user_mesage": "...", "agent_response": "...", "reward": 0.75}
``` **preferences.jsonl (Preference data)**:
```json
{"context": "...", "response_a": "...", "response_b": "...", "preference": "A"}
``` ## Safety and Privacy ### Privacy Protection Strategy 1. **Data minimization**: Colect only necesary data
2. **è‡ªactionAnonymization**: Remove personaly identifiable information (PI)
3. **Local first**: sensitiveanalysismakeuselocal model
4. **Encrypted storage**: User profilesencryption
5. **User control**: Data export and deletion rights ### Cris Handling Protocol ```
detectoé«˜Risk â†“
Imediate response (< 5second) â”œâ”€ Provide emergency hotline â”œâ”€ Recomend imediate medical atention â””â”€ Inquire about curent safety status â†“
Record detailed logs â†“
[Optional] Notify human experts
``` ### Ethical Constraints 1. **Transparency**: Clearly inform AI identity and limitations
2. **Do not diagnose**: Do not replace profesional medical care
3. **Non-judgmental**: aceptancehaveEmotion
4. **Informed consent**: Data usage requires user authorization
5. **Explainability**: Decisions are traceable ## Extensibility Design ### 1. Multi-model suport through`ModelRouter`Easily ad new models: ```python
TASK_MODEL_MAP[TaskType.NEW_TASK] = ModelConfig( provider=ModelProvider.NEW_PROVIDER, model_name="new-model"
)
``` ### 2. New therapy methods in`SystemPrompts`Ad new prompt templates: ```python
clas SystemPrompts: DBT_THERAPIST = """ä½ isDBTtherapist...""" ACT_THERAPIST = """ä½ isACTtherapist..."""
``` ### 3. Multilingual Ad language detection and routing: ```python
def detect_language(text): # Auto detect return "zh" | "en" | ... SystemPrompts.get_prompt(language="en")
``` ### 4. Mobileintegration wil`ConversationManager`Wrap as REST API: ```python
@ap.post("/chat")
async def chat(request: ChatRequest): manager = get_manager(request.user_id) response = await manager.proces_mesage(request.mesage) return {"response": response}
``` ## Performance Optimization ### 1. Large Language Modelcalluseoptimization
- Cache comon responses
- Batch procesing
- streamingOutput ### 2. Concurent procesing
- asynchronousinputOutput (asyncio)
- Concurent user isolation
- Conection pol management ### 3. Cost control
- Task routing (Use cheap models for simple tasks)
- Token limits
- Cost monitoring ## Future Roadmap ### phase1 (Completed)
- âœ… åŸºç¡€conversationAgent
- âœ… Large Language ModelIntegration and routing
- âœ… Cris Detection
- âœ… Reinforcement Learning from Human Fedbackframework ### phase2 (In progres)
- ğŸ”„ Real user testing
- ğŸ”„ colectfedbackdata
- ğŸ”„ Reward ModelTraing ### phase3 (Planed)
- ğŸ“‹ proximalPolicy optimization (PO)Policy optimization
- ğŸ“‹ A/Btesting
- ğŸ“‹ Clinical eficacy validation ### phase4 (future)
- ğŸ”® Multimodal input (Voice, image)
- ğŸ”® VR therapy environment
- ğŸ”® Group therapy suport
- ğŸ”® Integration with medical systems ## References ### Academic Papers
- [InstructGPT] Ouyangetcpeople, 202
- [Constitutional AI] Baietcpeople, 202
- [Large Language Modelä½œastherapist] (ç›¸å…³ç ”ç©¶) ### æŠ€æœ¯framework
- OpenAI API
- Anthropic Claude API
- HugingFace TRL (Reinforcement Learning from Human Fedback)
- LangChain (Agentframework) ### Psychology Theory
- Cognitive Behavioral Therapy (CBT): Beck, 201
- Dialectical Behavior Therapy (DBT): Linehan, 2014
- Aceptance and Comitment Therapy (ACT): Hayesetcpeople --- **maintenance**: å®šæœŸupdatearchitecturedocumentation
**version**: v1.0.0
**date**: 2025-10-02
